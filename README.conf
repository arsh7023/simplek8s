brew install kubectl
brew install minikube
minikube start
kubectl cluster-info

K8 objects:
Pods = runs one or more closely related containers

inside Pods can be docker - containers.

Master node , worker node - Pods inside pod

Nodes = individual machines or VMs that run containers.container.
Master = machine or VM manages nodes
Kubernetes = dont build our images , gets from somewhere else
To deploy, we update the desires state.

Config files = has Kind
kind: Pod


=====

K8 expects all images already built
one config file per object
we have to manually setup all networking

==========================================


if we run dokcer ps and kill a container, then minikube automativally creates a new container from docker hub.

deployment file (yml) files work with master.  if master told 4 running need, it makes sure 4 always is running.

==========================================

Objects = Pod - Service - Secrets -  Deployments (Stateful - Replica Controller)
Objects serves different purposes - running a container - monitoring a container - setting up - networking

Service = Used to setup some networking in Kubernetes cluster;

Service =  Ingress, NodePort, clusterIP, LoadBalancer:
-clusterIP = setup an easy-to rememeber URL to access a pod. Only exposes pods in the cluster. Exposes a set of pods to other objects in the cluster. provides access to/ in pods for everything only inside cluster
-nodePort= exposes a container to the outside world (only good for dev purposes)
-Ingress= better way to outside world!
-LoadBalancer: makes a POD accessible from outside the cluster. This is right way to expose a pod to outside world.

selector:
    component : web

Secrets = securely saves  information like passwords  in cluster

Pods = runs a single set of containers,  good for one-off dev rarely used in production

Deployment = runs a set of identical pods , monitor the state of each pod - good for dev - production.


clusterIP :
spec:
  type: ClusterIP
  selector:
    component: web
  ports:
    - port: 3000 == accessible for other POds inside cluster
      targetPort: 3000 = access to other pods running on the port 3000

==========================================


minikube start

kubectl cluster-info


kubectl apply -f client-pod.yaml
kubectl apply -f client-node-port.yaml

kubectl get pods

kubectl delete -f client-pod.yaml
kubectl delete -f client-node-port.yaml


==========================================

In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them
 (sometimes this pattern is called a micro-service).
  The set of Pods targeted by a Service is usually determined by a selector.
kubectl get services


minikube ip = find ip of k8 vm created by minikube
http://192.168.64.2:31515/
http://localhost:31515/  :if we use docker desktop and enabled kubernetes in it.

----update yml
--then get detailed info
kubectl describe pod client-pod

We are not allowed to update port in config.yaml file in pod. instead we use deployment

-update pods only allow for exampls image not port.
New type of Object:
Deployment :  kuberbets object maintains a set of identical pods.
==========================================

Pods= runs a single set of containers, good for one-off dev . not for prod

Deployment= runs a set of identical pods - monitors the state of each pod, good for dev, good for prod
replicas= number of pods using temlate written

kind: Deployment

kubectl apply -f client-deployment.yaml
kubectl get deployments

kubectl get pods -o wide
every pod has a single ip
==========================================

delete = kubectl delete -f client-pod.yaml

Service = connects the port to Pods ipaddress so we dont need to worry about the changing of pods ipaddess everytime it changes.
==========================================

update image for kub deplot,ent
docker build -t  arsh7023/mlti-client:v5 .
docker push== docker push to docker hub
docker push arsh7023/mlti-client

--impetartive
kubectl set image deployment/client-deployment client=arsh7023/multi-client:v5   ==> client-deployment is the container name of the container we are updating, get from config file.
kubectl get pods

==========================================
If you want to reconfigure your docker cli to comminuicate to docker server inside a virtaul machine:
eval ($minikube docker-env) == config the vm to use your docker server
This only configure your current terminal window;

munikube docker-env:  shows docker settings inside minikube cluster

==========================================

kubectl get deployments
kubectl delete deployment client-deployment

kubectl apply -f foldername = apply all files inside folder
kubectl apply -f k8s == look at complex-stephen project

kubectl get services
kubectl delete service client-node-port
==========================================
kubectl get pods = get id
kubectl logs id => this is same: 1)docker ps  2) get id  3) docker id logs

docker commands mostly avaiable via kubectl

==========================================
production= kubernetes

inside complec-stephen folder run:
docker-compose up --build  => build everything

crearte yaml files in k8s directory


==========================================
volume =  deletes when pods delets
volume in k8 = An object that allows a container to store data at the pod level.
volume in k8 is different from docker volume as it gets deleted when pods kills.



persistent volume = is outside pod

PersistenceVolumeClaim  = PVC = is like ads - the existing inventory. 500gb , the volumes created a head of time.
if k8 doesnot have a pvc is creates on the fly

database-persistent-volume-claim.yaml
 storage: 2Gi = 2gb space

ReadWriteOnce = can be used by a single node
ReadOnlyMany = Multiple nodes can read
ReadWriteMany = can be read and written to by many nodes



accessMode s21.png

kubectl get storageclass  = options for k8 in computer to find a persistent volume.
kubectl describe storageclass

kubectl get pv : persistent volumes
kubectl get pvc : persistent volumes claims

postgres-deployment.yaml=
     volumes:
        - name: postgres-storgae
          persistentVolumeClaim:
            claimName: database-persistent-volume-claim


database-persistent-volume-claim.yaml=
metadata:
  name: database-persistent-volume-claim


    volumeMounts:
      - name: postgres-storgae
        mountPath: /var/lib/postgresql/data
        subPath: postgres

==========================================
environemnt variable =
worker-deployment.yaml =
 env:
            - name: REDIS_HOST
              value: redis-cluster-ip-service
            - name: REDIS_PORT
              value: '6379'
==========================================
Secrets: secure information like passwords
kubectl create secret generic pgpassword --from-literal PGPASSWORD=12345asdf


kubectl get secrets
==========================================
loadbalancer : legacy
ingress = new
igress-nginx community version s29.png
https://github.com/kubernetes/ingress-nginx

controller:
in k8 a controller is any type of  object that constantly works to meet a desire state of a reality

https://www.joyfulbikeshedding.com/blog/2018-03-26-studying-the-kubernetes-ingress-system.html
https://kubernetes.github.io/ingress-nginx/deploy/#prerequisite-generic-deployment-command

 command = minikube dashboard

 kubectl get namespaces = by default k8 crreats a namespace

 =============

 Helm = client = cli
 tiller = server
 command => hel m clienr => tiller serrver which is in a pod.

 RBAC = role based access control. => limits who can access and modify objects

 google cloud enambedl

 tiller = modify changes to cluster
 make sure tiller has ability to access and modify


 user accounts: person
 service accounts: identify a "pod" adminstaring a cluster
 clusterRoleBiniding: authorise an account to do a certain actions across to entire cluster
 RoleBiniding = authorise an account to do a certain actions across in a sinle namespace

 Assigning a tiller a service account:
 kubectl create serviceaccount --namespace kube-sysyem tiller
 kubectl create clusterRoleBiniding tiller-cluster-role ....

 tiller init    --service-account tiller --upgrade

 heml install stable/nginx

 =====

 Build  project udemy stephen

 basic-yaml
1 - build: client ashamakhy$ docker build -t arsh7023/multi-client .   ==> builds the docker image
2- pish to docker-hub: docker push arsh7023/multi-client

docker images:  lists all docker

docker login => arsh7023-Poo

docker ps
doccker logs dockerid
==
docker system prune -a :  deletes every container

=====build complex

inside complex:

cd complex = docker-compose up --build

http://localhost:3050/

======

kubectl get pods : posts
kubectl exec -it posts sh
kubectl logs posts == posts is a name of the pod

kubectl describe pod posts
kubectl delete pod posts

----

Deployments exampls: It can run multiple Pods(Containers)
In below all of the pods in this deployment have a label of app with a value of posts

posts-depl.yaml:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: posts-depl
spec:
  replicas: 1
  selector:
    matchLabels:
      app: posts
  template:
    metadata:
      labels:
        app: posts
    spec:
      containers:
        - name: posts
          image: arsh7023/posts:0.0.1

-- kubectl apply -f posts-depl.yaml
kubectl describe deployments posts-depl

--if config changes rerun:
kubectl apply -f posts-depl.yaml

-- if lets say you change image: arsh7023/posts:0.0.1 in deployment file to image: arsh7023/posts:latest and want to pods in deployment gets updated.
kubectl rollout restart deployment posts-depl.yaml

Service

apiVersion: v1
kind: Service
metadata:
  name: posts-srv
spec:
  type: NodePort
  selector:
    app: posts
  ports:
    - name: posts
      protocol: TCP
      port: 4000
      targetPort: 4000


selector is telling that this service which is a type of NodePort enables neteworking for the pods wich tagged by app:posts , to be accessible  outside world. NodePort is good for developmemt
since we have only one POD

tagetPort means that the POD (which tagged by app:posts) is runnig on port 4000 (since post application is listening on port 4000 in the code)
But the port for external which this service wants to provide for other pods is 4000 ( it can be differebt from targetPort).

kubectl apply -f posts-srv.yaml


kubectl get services
kubectl describe service posts-srv


if use minikube :
minikube ip

if docker mac:
localhost:31088/posts

docker build -t arsh7023/event-bus .
 docker push arsh7023/event-bus == push to docker hub

 ------

 ClusterIP : is we dont say "type: ClusterIP" below, k8 consider by default clusterIP:

 apiVersion: v1
 kind: Service
 metadata:
   name: event-bus-srv
 spec:
   selector:
     app: event-bus
   type: ClusterIP
   ports:
     - name: event-bus
       protocol: TCP
       port: 4005
       targetPort: 4005


form one pod to another pod if we want to access we can call:
http://event-bus-srv:4005
http://posts-clusterip-srv:4000

 kubectl rollout restart deployment posts-depl
  kubectl rollout restart deployment events-bus-depl

kubectl logs posts-depl-856b4bf747-795rm


apply all configs inside current directory
 kubectl apply -f .

 Ingress === A Pod with a set of routing rules to distribute traffic to other services.
 LoadBalancer === Tells K8 to reach out to its provider and provision a load balancer. Gets traffic to a single Pod.

 We are using ingress-nginx NOT kubernetes-ngnix
https://kubernetes.github.io/ingress-nginx/deploy/#docker-for-mac

 kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-0.32.0/deploy/static/provider/cloud/deploy.yaml


kubectl create secret generic jwt-secret --from-literal=JWT_KEY=asdf

kubectl get secrets
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-depl
spec:
  replicas: 1
  selector:
    matchLabels:
      app: auth
  template:
    metadata:
      labels:
        app: auth
    spec:
      containers:
        - name: auth
          image: arsh7023/auth
          env:
            - name: JWT_KEY
              valueFrom:
                secretKeyRef:
                  name: jwt-secret
                  key: JWT_KEY



===============

kubectl get namespaces:
  docker
  ingress-ngnix
  **

Then:

kubectl get services -n ingress-nginx ==> -n : namespace

====

kubectl port-forward gateway-6884b7656b-x9f8q 30100:80